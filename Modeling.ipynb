{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b4136-5529-4354-868f-52200f4861c2",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fea294-caa6-486c-8209-bf80e21f90a7",
   "metadata": {},
   "source": [
    "In this section, I performed the following:\n",
    "* **Split the dataset into training and testing sets:** The dataset is divided into training (80%) and testing (20%) sets to ensure models are trained on one portion and tested on unseen data.\n",
    "* **Train baseline models (Logistic Regression):** A simple Logistic Regression model is trained first as a benchmark to compare against more complex models.\n",
    "* **Train advanced models (Random Forest, XGBoost):** More powerful models like Random Forest (ensemble-based) and XGBoost (gradient boosting) are trained to improve accuracy and performance.\n",
    "* **Perform Cross-Validation:** Use cross-validation to assess model performance across different splits of the dataset.\n",
    "* **Evaluate models and save the best-performing one:** Models are assessed using metrics such as classification report, confusion matrix, and AUC-ROC score. The best-performing model is saved for later use in deployment.\n",
    "* **Plot Confusion Matrix for Best Models on Test Set:** Generate and visualize the confusion matrix for the best-performing model to analyze predictions.\n",
    "* **Interactive Dashboard:** Interactive Dashboardthree tuned machine learning models (Logistic Regression, Random Forest, and XGBoost), computes classification metrics and AUC-ROC scores, and stores the results for visualization in an interactive dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961a1381-cf2d-4534-a8f2-400c6fce12af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e21ef-cbc2-47bb-ad62-b78e22ba009a",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a056e-fd93-4a4e-95bd-615bceaab1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=500)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f16d3-72cd-464c-b7a5-5ae6e5aee5ec",
   "metadata": {},
   "source": [
    "### Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba434520-f0e5-457c-a9f1-ccabc53a6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9210c5-d36e-47c6-931d-2ac0853a92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1aaa0-0cdb-45f6-88fd-38ddc4a0774f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48ef47-0b7f-4ace-b6df-229c4b69172f",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370358e-192e-471a-8bcb-7ae3a04ca4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Hyperparameter Tuning using GridSearchCV\n",
    "logistic_params = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l2']}\n",
    "logistic_grid = GridSearchCV(LogisticRegression(max_iter=1000), logistic_params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "logistic_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9a2e5-e327-44cd-afa1-67795556814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best parameters\n",
    "print(f\"Best Parameters: {logistic_grid.best_params_}\")\n",
    "\n",
    "# Display the best score\n",
    "print(f\"Best ROC-AUC Score: {logistic_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6ab5e-ef26-4acb-aa5a-e88b7f450e24",
   "metadata": {},
   "source": [
    "**RandomForest Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dddc3-8bb4-4991-afba-09d10a17f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Hyperparameter Tuning using RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter search space\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    cv=3,       # Reduce folds to speed up tuning\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters and best score\n",
    "print(f\"Best Parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best ROC-AUC Score: {rf_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee768b-9efe-42b9-baae-403a62a3ed99",
   "metadata": {},
   "source": [
    "**xbg Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f51af-e96c-4192-b2ec-a412c3e30e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xbg Hyperparameter Tuning using RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter search space\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluation metric\n",
    "    cv=5,  # Cross-validation folds\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42,\n",
    "    verbose=2  # Optional: Shows progress during tuning\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and best score\n",
    "print(f\"Best Parameters: {xgb_random.best_params_}\")\n",
    "print(f\"Best ROC-AUC Score: {xgb_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21029b-401c-4bca-b8c5-b4378aa5a042",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d273e-c3fc-43b8-8bc6-1c8fcc9de706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Cross-Validation\n",
    "def evaluate_with_cross_validation(model, X_train, y_train):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print(f\"Cross-Validation AUC-ROC Scores: {scores}\")\n",
    "    print(f\"Mean AUC-ROC Score: {scores.mean():.4f}\\n\")\n",
    "    \n",
    "# Cross-validate each tuned model\n",
    "print(\"Logistic Regression Cross-Validation:\")\n",
    "evaluate_with_cross_validation(logistic_grid.best_estimator_, X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Cross-Validation:\")\n",
    "evaluate_with_cross_validation(rf_grid.best_estimator_, X_train, y_train)\n",
    "\n",
    "print(\"XGBoost Cross-Validation:\")\n",
    "evaluate_with_cross_validation(xgb_grid.best_estimator_, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a15158-2115-4b4c-a691-cdd6c6b5be97",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix for Best Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b157b0-1b53-40be-b797-991185448c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for Best Models on Test Set\n",
    "for model, name in zip(\n",
    "    [logistic_grid.best_estimator_, rf_grid.best_estimator_, xgb_grid.best_estimator_],\n",
    "    ['Logistic Regression (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)']\n",
    "):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Print classification metrics\n",
    "    print(f\"{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_proba):.4f}\\n\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Save Confusion Matrices to CSV Files\n",
    "for model, name in zip(\n",
    "    [logistic_grid.best_estimator_, rf_grid.best_estimator_, xgb_grid.best_estimator_],\n",
    "    ['logistic', 'rf', 'xgb']\n",
    "):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Not Fraud\", \"Fraud\"], columns=[\"Not Fraud\", \"Fraud\"])\n",
    "    cm_df.to_csv(f\"confusion_matrix_{name}.csv\", index=False)\n",
    "\n",
    "print(\"Confusion matrices saved as CSV files!\")\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Initialize an empty list to store metrics\n",
    "metrics = []\n",
    "\n",
    "# Evaluate each model and collect metrics\n",
    "for model, name in zip(\n",
    "    [logistic_grid.best_estimator_, rf_grid.best_estimator_, xgb_grid.best_estimator_],\n",
    "    ['Logistic Regression (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)']\n",
    "):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    # Append metrics to the list\n",
    "    metrics.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision\": report['1']['precision'],  # For the positive class (fraud)\n",
    "        \"Recall\": report['1']['recall'],        # For the positive class (fraud)\n",
    "        \"F1-Score\": report['1']['f1-score'],    # For the positive class (fraud)\n",
    "        \"ROC-AUC\": auc_roc\n",
    "    })\n",
    "\n",
    "# Convert metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_df.to_csv(\"model_metrics.csv\", index=False)\n",
    "\n",
    "print(\"Model metrics saved as model_metrics.csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b084dfd-ec53-4cdc-832c-5e183c03f352",
   "metadata": {},
   "source": [
    "## BUILDING INTERACTIVE DASHBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9df31-295a-4014-9d8b-09eb9fa137e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Initialize an empty list to store metrics\n",
    "metrics = []\n",
    "\n",
    "# Evaluate each model and collect metrics\n",
    "for model, name in zip(\n",
    "    [logistic_grid.best_estimator_, rf_grid.best_estimator_, xgb_grid.best_estimator_],\n",
    "    ['Logistic Regression (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)']\n",
    "):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    # Append metrics to the list\n",
    "    metrics.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision\": report['1']['precision'],  # For the positive class (fraud)\n",
    "        \"Recall\": report['1']['recall'],        # For the positive class (fraud)\n",
    "        \"F1-Score\": report['1']['f1-score'],    # For the positive class (fraud)\n",
    "        \"ROC-AUC\": auc_roc\n",
    "    })\n",
    "\n",
    "# Convert metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_df.to_csv(\"model_metrics.csv\", index=False)\n",
    "\n",
    "print(\"Model metrics saved as model_metrics.csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde5f69-26f0-4a3e-87ca-eaf7fc595da0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The objective of predicting credit card payment defaults was analyzed using Logistic Regression, Random Forest, and XGBoost models. Below are the key takeaways:\n",
    "\n",
    "### Logistic Regression:\n",
    "- **Strengths:** High recall for non-defaults (0.97) and reasonable AUC-ROC score (0.7103).\n",
    "- **Weaknesses:** Poor recall for defaults (0.24), missing many defaulting customers.\n",
    "\n",
    "### Random Forest:\n",
    "- **Strengths:** Improved recall for defaults (0.35) and a better AUC-ROC score (0.7750).\n",
    "- **Weaknesses:** Still struggles with default recall and slightly more false positives.\n",
    "\n",
    "### XGBoost:\n",
    "- **Strengths:** Best AUC-ROC score (0.7795) and improved precision for defaults (0.61).\n",
    "- **Weaknesses:** Recall for defaults remains low (0.37), slightly higher than Random Forest.\n",
    "\n",
    "### Key Insight:\n",
    "All models effectively identify non-defaulting customers but struggle with recall for defaults, which is critical for mitigating credit risk. XGBoost shows the best overall performance but requires further optimization to improve default classification.\n",
    "\n",
    "### Recommendations:\n",
    "- **Enhance feature engineering** (e.g., derive new features from payment history).\n",
    "- **Balance the dataset** (e.g., using SMOTE or cost-sensitive learning).\n",
    "- **Adjust classification thresholds** to improve recall for defaults while maintaining overall accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
